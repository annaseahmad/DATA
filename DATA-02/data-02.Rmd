---
title: '[DATA-02] Linear regression'
author: "Miguel-Angel Canela, IESE Business School"
date: "June 21, 2016"
output:
  word_document:
    toc: no
  pdf_document:
    toc: no
  html_document:
    theme: united
    toc: no
---
### Introduction

In a data science context, the prediction of a numeric variable is called regression. Regression models are not necessarily related to a mathematical equation, as in Statistics, although, this is the favourite approach for beginners. When this equation is linear, we have **linear regression**, which is the object of this lecture. Although the predictions of a linear regression model can usually be improved by more sophisticated techniques, we always start there, because it helps us to understand the data.

Two alternatives to linear regression are:

* **Regression trees**. In this course, tree algorithms are discusused in a classification context, but some of them, like the CART algorithm used by with the package `rpart`, apply also to regression trees. 

* **Neural networks**. A neural network is a programming device whose design is based on the models developed by biologists for the brain neurons. Among the many types of neural networks, the most popular is the **multilayer perceptron** (MLP), which can be regarded as a set of (nonlinear) regression equations. In R, the package `nnet` provides a simple approach to MLP regression models. 

### Evaluation of a linear regression model

In general, regression models are evaluated through the prediction errors. The basic schema is
$${\rm Error}={\rm Actual\ value}-{\rm Predicted\ value}.$$

Prediction errors are called **residuals** in linear regression. In this special case, the mean of the prediction errors is zero, but this is no longer true in other models. The usual approach for estimating the regression coefficients, the **least squares method**, minimizes the sum of the squared residuals. 

In Statistics, the predictive power of a linear regression model is evaluated through the **residual sum of squares**. The **R-square statistic** is a standardized measure which operationalizes this. More specifically, it takes advantage of the formula
$${\rm var}\big({\rm Actual\ values}\big)={\rm var}\big({\rm Predicted\ values}\big)+
  {\rm var}\big({\rm Error}\big),$$
to evaluate the model through the proportion of **variance explained**,
$$R^2={{\rm var}\big({\rm Predicted\ values}\big)\over
  {\rm var}\big({\rm Actual\ values}\big)}\,.$$

It turns out that the square root of R-squared coincides with the **correlation** between actual values and predicted values. Although this stops being true for other regression methods, this correlation is still the simplest method for a fast and dirty evaluation of a regression model.

### Dummies and factors

**Categorical variables** (also called nominal) enter a regression equation through 1/0 variables, called **dummies**. In R, we can manage this in two ways.

* We can create dummies explicitly and include them in the equation (as many dummies as the number of categories minus 1). 

* We can specify the variable as a factor and put it in the formula. Then R creates the dummies itself, without need of changing the data set.

### Transformations

Transformations, such the square root or the logarithm, are recommended in Statistics textbooks in many situations. The only transformation that we may use in this course is the **log transformation**, so my comments are restricted to this case. Let me start by refreshing some basic facts.

In R, `log` denotes the **natural logarithm** (it is `ln in Excel). This function is the inverse of the exponential formula, meaning that
$$y=\log(x)\ \Longleftrightarrow\ \exp(y)=x.$$

An important property of the logarithm is that it transforms products into sums and quotients into differences,
$$\log\big(x_1x_2\big)=\log(x_1)+\log(x_2),\qquad 
  \log\left(x_1\over x_2\right)=\log(x_1)-\log(x_2).$$

The use of the log transformation can be supported by various arguments: 

* Limiting the influence of extreme observations.

* Rendering assumptions about normality more reliable. This applies only to the dependent variable in a testing context. 

* When an independent variable has an effect on the dependent variable which is stronger for low values. For instance, the effect on your influence of adding a political contact to your portfolio is strong for the first contacts, but gets weaker and weaker when you already have many contacts.

Is this useful in a data mining context? In data mining the focus is on prediction. So, we are pragmatic. If a transformation improves our prediction, it is welcome. As shown in the example, it is easy to check wether the log transformation of the price improves the model. 

### Example: Windsor housing prices

In this example, I develop a pricing model for residential houses for the area of Windsor, Ontario. The data set, provided by the Windsor and Essex County Real Estate Board, covers the sales of residential houses in Windsor during July, August and September of the previous year through the Multiple Listing Service. The sample size is 546.

Besides the sale price, the data set contains information describing the key features of each house: 

* The lot size, which should play a strong role in the model.

* The number of bedrooms. 

* The number of full bathrooms (including at least the toilet, the sink and the bathtub).

* The number of stories, excluding the basement.

* The number of garage places. 

* A dummy for having a recreational room.

* A dummy for having a full and finished basement.

* A dummy for using gas for hot water heating.

* A dummy for having central air conditioning.

* A dummy for having a driveway.

* A dummy for being located in a preferred neighborhood of the city (Riverside or South Windsor).

Most of the data sets used in this course come in csv files. Theese are text files that use the comma as the column separator. This format is very popular, although it can lead to errors with text data. The names of the attributes are in the first row, and every other row corresponds to an instance.

```{r, echo=F} 
setwd("/Users/miguel/Dropbox (Personal)/Current jobs/DATA-2016-2/DATA-02")
```{r}
windsor <- read.csv(file="windsor.csv")
```

Please, note that, where I have written `"windsor.csv"`, you have to write the complete path of this file in your computer, for the file to be found by R. As I am writing it, my code will work only if the file is in the **working directory**. You can change the working directory with the function `setwd`. Actually, `windsor` is a data frame, with 546 and 11 columns.

```{r}
str(windsor)
```

A linear regression model can be obtained with the function `lm`. The key arguments are `formula` and `data`, of obvious meaning. The syntax of the formula is `y ~ x1 + x2 + ...`

```{r}
fm1 <- price ~ lotsize + nbdrm + nbhrm + nstor + ngar + drive + recrm + base + 
  gas + air + neigh
mod1 <- lm(formula=fm1, data=windsor)
```

`mod1` is an object of the class `list`. A **list** can be understood as a bag containing objects of different type. The same will true for other models used in this course, but I will not discuss what the content is for the different models discussed in this course, because that would be too technical. If you are interested in the structure of an R object, you can explore it the function `names` or (at your own risk) with `str`, which provides a lot of information but can give you too much output for complex objects. 

```{r}
names(mod1)
```

In most models, the function `summary` provides useful information. In this case it includes the regression coefficients, with the corresponding **p-values**, and the R-squared statistic, whose square root, R = 0.820, is the correlation between actual and predicted prices. 

The function `summary` also provides a summary of the residuals. Do not look at the maximum and minimum, since they depend on extreme observations. But the median (50% percentile), and the first (25% percentile) and third quartiles (75% percentile) can be useful. 

```{r}
summary(mod1)
```

The predicted values of a linear regression model are actually contained in the model, but I use here the function `predict` in order to apply the same steps as with other models. `predict` can be applied to a new data set, as far as it contains all the variables included on the right side of the formula of the model. 

```{r}
pred1 <- predict(object=mod1, newdata=windsor)
```

We can also examine directly the prediction errors, that is, the regression residuals,

```{r}
res1 <- windsor$price - pred1
```

The standard deviation of the residuals is 27,807.30. Although the correlation achieved with this regression equation may seem very high, the residual standard deviation is still the 57.2% of that of the price (48,364.55). 

```{r}
sd(res1)
sd(windsor$price)
sd(res1)/sd(windsor$price)
```

Another way of assessing the performance of the model is through the percentage of cases in which the error exceeds a given threshold. For instance, in this case only 12.8% of the prediction errors exceed 40,000 dollars (in absolute value), while 22.5% exceed 30,000. This allows for an assessment in dollar terms which is always helpful and may bridge the gap between the analyst and a non-trained audience. 

To get this proportion, I write the expression that I want to evaluate, `abs(res1)>40000`. This creates a logical vector (`TRUE/FALSE`). When I apply `mean`, R transforms this into a dummy (`1/0`), so the mean gives the proportion of `TRUE`values. 

```{r}
mean(abs(res1)>40000)
```

Although the prices have a **skewed distribution**, that of the residuals is reasonably
symmetric, as shown in the exhibit below, obtained by means of

```{r}
par(mfrow=c(1,2))
hist(windsor$price, main="", xlab="Price")
hist(res1, main="", xlab="Prediction error")
```

The command `par(mfrow=c(1,2))` splits the graphic device in two parts, so the next two plots will fill the two slots. With the argument `mfrow` we control the partition. The `mfrow=c(1,2)` specification means "one row, two columns". You may resize the window in your screen top take advantage of this. Note that this split is great for the R console, but can produce a poor result in RStudio.

### Log transformation

The distribution of the price is skewed, but not severely. So, I don't expect a big change from using log scale for prices. Nevertheless, I try it as an illustration. In R, logarithms can be introduced directly in the formula.

```{r}
fm2 <- log(price) ~ lotsize + nbdrm + nbhrm + nstor + ngar + drive + recrm + base +
  gas + air + neigh
mod2 <- lm(formula=fm2, data=windsor) 
summary(mod2)
```

Now, the summary should be taken with care, since it is referred to prices in log scale. For instance, the summary of the residuals is not useful, unless we transform these values into prices with an exponential transformation. Indeed, the relationship
$${\rm Residual}=\log\big({\rm Actual\ price}\big)-\log\big({\rm Predicted\ price}\big)$$
becomes
$$\exp\big({\rm Residual}\big)={{\rm Actual\ price}\over{\rm Predicted\ price}}\,.$$

So, the percentiles for the prediction errors tell us that, 25% of the predicted prices are below of 88.5, 1.6 and 13.7%, respectively, of the actual prices.

```{r}
exp(-0.12226)
exp(0.01633)
exp(0.12873)
```

The same with the correlation. The R-squared value reported is practically the same in the two models. But, if we want the right comparison, we have to calculate the correlation between predicted prices and actual prices. The correlation is also practically the same.

```{r}
pred2 <- exp(predict(mod2, newdata=windsor))
cor(windsor$price, pred2)
```

The calculations and graphics prepared for the linear model can be easily reproduced for this alternative model. We have now 11% of the instances with an error less than 40,000 dollars. So far, the improvement observed does not support the log transformation (it can in other examples).
The left part of the exhibit shows how the skewness is corrected by the log transformation. 

```{r}
res2 <- windsor$price - pred2
sd(res2)
sd(res2)/sd(windsor$price)
mean(abs(res2)>40000)
par(mfrow=c(1,2))
hist(log(windsor$price), main="", xlab="Price (log scale)")
hist(res2, main="", xlab="Prediction error")
```

